[{"pretty_name": "benchmarks.AbsenteeismSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": 4.4317347294575455, "change_rev": [39607, 39625], "idx": 0, "name": "benchmarks.AbsenteeismSuite.track_mae", "last_rev": 39625, "last_value": 3.8552355250796757}, {"pretty_name": "benchmarks.AbsenteeismSuite.track_mae('automl_gs')", "last_err": 0.008895209243705793, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AbsenteeismSuite.track_mae", "last_rev": 39625, "last_value": 4.440629938701251}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('nps')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.6666666683943375}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('paper-reviews')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.7804878048780488}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('user-knowledge-modeling')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.9615384340286255}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('wine')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('nps')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.5219512195121951}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('paper-reviews')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.8442622950819673}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('user-knowledge-modeling')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.9743589743589745}, {"pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('wine')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.AirflowSelfNoiseSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": 4.122581357156624, "change_rev": [39470, 39625], "idx": 0, "name": "benchmarks.AirflowSelfNoiseSuite.track_mae", "last_rev": 39625, "last_value": 4.5573069117716605}, {"pretty_name": "benchmarks.AirflowSelfNoiseSuite.track_mae('automl_gs')", "last_err": 0.35375554074434623, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AirflowSelfNoiseSuite.track_mae", "last_rev": 39625, "last_value": 3.9181944368958734}, {"pretty_name": "benchmarks.AutismScreeningSuite.track_precision('appsheet')", "last_err": 0.0, "prev_value": 1.0, "change_rev": [39607, 39625], "idx": 0, "name": "benchmarks.AutismScreeningSuite.track_precision", "last_rev": 39625, "last_value": 0.95}, {"pretty_name": "benchmarks.AutismScreeningSuite.track_precision('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AutismScreeningSuite.track_precision", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.AutismScreeningSuite.track_recall('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.AutismScreeningSuite.track_recall", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.AutismScreeningSuite.track_recall('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AutismScreeningSuite.track_recall", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.AutoMpgSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.AutoMpgSuite.track_mae", "last_rev": 39625, "last_value": 2.630856685149364}, {"pretty_name": "benchmarks.AutoMpgSuite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.AutoMpgSuite.track_mae", "last_rev": 39625, "last_value": 2.17774829864502}, {"pretty_name": "benchmarks.ChurnPredictionSuite.track_precision('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.ChurnPredictionSuite.track_precision", "last_rev": 39625, "last_value": 0.8541666666666666}, {"pretty_name": "benchmarks.ChurnPredictionSuite.track_precision('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.ChurnPredictionSuite.track_precision", "last_rev": 39625, "last_value": 0.9420889348500516}, {"pretty_name": "benchmarks.ChurnPredictionSuite.track_recall('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.ChurnPredictionSuite.track_recall", "last_rev": 39625, "last_value": 0.7592592592592593}, {"pretty_name": "benchmarks.ChurnPredictionSuite.track_recall('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.ChurnPredictionSuite.track_recall", "last_rev": 39625, "last_value": 0.6905827787860456}, {"pretty_name": "benchmarks.ConcreteCalculatorSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": 5.886489383676678, "change_rev": [39470, 39625], "idx": 0, "name": "benchmarks.ConcreteCalculatorSuite.track_mae", "last_rev": 39625, "last_value": 7.354617081799553}, {"pretty_name": "benchmarks.ConcreteCalculatorSuite.track_mae('automl_gs')", "last_err": 1.3628016502314528, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.ConcreteCalculatorSuite.track_mae", "last_rev": 39625, "last_value": 8.286293839945376}, {"pretty_name": "benchmarks.DressesSalesSuite.track_precision('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.DressesSalesSuite.track_precision", "last_rev": 39625, "last_value": 0.4473684210526316}, {"pretty_name": "benchmarks.DressesSalesSuite.track_precision('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.DressesSalesSuite.track_precision", "last_rev": 39625, "last_value": 0.5716501240694789}, {"pretty_name": "benchmarks.DressesSalesSuite.track_recall('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.DressesSalesSuite.track_recall", "last_rev": 39625, "last_value": 0.9444444444444444}, {"pretty_name": "benchmarks.DressesSalesSuite.track_recall('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.DressesSalesSuite.track_recall", "last_rev": 39625, "last_value": 0.5528188286808977}, {"pretty_name": "benchmarks.EnergyEfficiency1Suite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.EnergyEfficiency1Suite.track_mae", "last_rev": 39625, "last_value": 2.549680010064856}, {"pretty_name": "benchmarks.EnergyEfficiency1Suite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.EnergyEfficiency1Suite.track_mae", "last_rev": 39625, "last_value": 1.7529308504046817}, {"pretty_name": "benchmarks.EnergyEfficiency2Suite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.EnergyEfficiency2Suite.track_mae", "last_rev": 39625, "last_value": 2.4776498373452718}, {"pretty_name": "benchmarks.EnergyEfficiency2Suite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.EnergyEfficiency2Suite.track_mae", "last_rev": 39625, "last_value": 1.3987778264825992}, {"pretty_name": "benchmarks.HeartDiseaseSuite.track_precision('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.HeartDiseaseSuite.track_precision", "last_rev": 39625, "last_value": 0.7391304347826086}, {"pretty_name": "benchmarks.HeartDiseaseSuite.track_precision('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.HeartDiseaseSuite.track_precision", "last_rev": 39625, "last_value": 0.8164556962025317}, {"pretty_name": "benchmarks.HeartDiseaseSuite.track_recall('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.HeartDiseaseSuite.track_recall", "last_rev": 39625, "last_value": 0.8947368421052632}, {"pretty_name": "benchmarks.HeartDiseaseSuite.track_recall('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.HeartDiseaseSuite.track_recall", "last_rev": 39625, "last_value": 0.8090243902439025}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('absenteeism')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 3.8552355250796757}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('airflow-self-noise')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 4.5573069117716605}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('auto-mpg')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 2.630856685149364}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('concrete-calculator')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 7.354617081799553}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('energy-efficiency-1')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 2.549680010064856}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('energy-efficiency-2')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 5, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 2.4776498373452718}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('student-performance-math')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 6, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 1.8844806790351867}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('student-performance-por')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 7, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 0.6685541006234976}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('wine-quality')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 8, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 0.4875065803527832}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('yacht-hydrodynamics')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 9, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 2.3854258060455322}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('absenteeism')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 4.449525147944957}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('airflow-self-noise')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 3.7958809229814814}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('auto-mpg')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 2.17774829864502}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('concrete-calculator')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 10.233697015212963}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('energy-efficiency-1')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.7529308504046817}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('energy-efficiency-2')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 5, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.3987778264825992}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('student-performance-math')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 6, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 4.42217900259655}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('student-performance-por')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 7, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.3589764619484923}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('wine-quality')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 8, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.5111147334178289}, {"pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('yacht-hydrodynamics')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 9, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 2.0546305096533994}, {"pretty_name": "benchmarks.NpsSuite.track_acc('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.NpsSuite.track_acc", "last_rev": 39625, "last_value": 0.6666666683943375}, {"pretty_name": "benchmarks.NpsSuite.track_acc('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.NpsSuite.track_acc", "last_rev": 39625, "last_value": 0.5219512195121951}, {"pretty_name": "benchmarks.PaperReviewsSuite.track_acc('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.PaperReviewsSuite.track_acc", "last_rev": 39625, "last_value": 0.7804878048780488}, {"pretty_name": "benchmarks.PaperReviewsSuite.track_acc('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.PaperReviewsSuite.track_acc", "last_rev": 39625, "last_value": 0.8442622950819673}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.95}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.8541666666666666}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.4473684210526316}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.7391304347826086}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.717391304347826}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.9420889348500516}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.5716501240694789}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.8164556962025317}, {"pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.8185328185328185}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.7592592592592593}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.9444444444444444}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.8947368421052632}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 0.9166666666666666}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.6905827787860456}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.5528188286808977}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.8090243902439025}, {"pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 0.7222418358340689}, {"pretty_name": "benchmarks.StudentPerformanceMathSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.StudentPerformanceMathSuite.track_mae", "last_rev": 39625, "last_value": 1.8844806790351867}, {"pretty_name": "benchmarks.StudentPerformanceMathSuite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.StudentPerformanceMathSuite.track_mae", "last_rev": 39625, "last_value": 4.42217900259655}, {"pretty_name": "benchmarks.StudentPerformancePorSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.StudentPerformancePorSuite.track_mae", "last_rev": 39625, "last_value": 0.6685541006234976}, {"pretty_name": "benchmarks.StudentPerformancePorSuite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.StudentPerformancePorSuite.track_mae", "last_rev": 39625, "last_value": 1.3589764619484923}, {"pretty_name": "benchmarks.TitanicSuite.track_precision('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TitanicSuite.track_precision", "last_rev": 39625, "last_value": 0.717391304347826}, {"pretty_name": "benchmarks.TitanicSuite.track_precision('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TitanicSuite.track_precision", "last_rev": 39625, "last_value": 0.8185328185328185}, {"pretty_name": "benchmarks.TitanicSuite.track_recall('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TitanicSuite.track_recall", "last_rev": 39625, "last_value": 0.9166666666666666}, {"pretty_name": "benchmarks.TitanicSuite.track_recall('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TitanicSuite.track_recall", "last_rev": 39625, "last_value": 0.7222418358340689}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 32.047656297683716}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 43.55696892738342}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 31.207871198654175}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 28.1053524017334}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 27.223865509033203}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 2609.7796580791473}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 2399.239366531372}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1567.8775324821472}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 2765.232954263687}, {"pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('titanic')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 2645.2976908683777}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('nps')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 28.66774344444275}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('paper-reviews')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 30.3750216960907}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('user-knowledge-modeling')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 29.761793851852417}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('wine')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "last_rev": 39625, "last_value": 28.168158531188965}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('nps')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1141.703991651535}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('paper-reviews')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1846.0559742450714}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('user-knowledge-modeling')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 1324.8651053905487}, {"pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('wine')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "last_rev": 39625, "last_value": 2235.147046804428}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('absenteeism')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 23.939385414123535}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('airflow-self-noise')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 39.72560095787048}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('auto-mpg')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 30.665636777877808}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('concrete-calculator')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 23.945170402526855}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('energy-efficiency-1')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 30.47210931777954}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('energy-efficiency-2')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 5, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 23.35370922088623}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('student-performance-math')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 6, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 28.126938819885254}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('student-performance-por')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 7, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 29.535085916519165}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('wine-quality')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 8, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 32.21105217933655}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('yacht-hydrodynamics')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 9, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "last_rev": 39625, "last_value": 28.3132061958313}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('absenteeism')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1720.7366738319397}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('airflow-self-noise')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1000.5141520500183}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('auto-mpg')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 2, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 2555.131772994995}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('concrete-calculator')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 3, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1688.6441690921783}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('energy-efficiency-1')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 4, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1253.6088423728943}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('energy-efficiency-2')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 5, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1532.7790684700012}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('student-performance-math')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 6, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1044.400455713272}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('student-performance-por')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 7, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 686.2359955310822}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('wine-quality')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 8, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 1842.6598403453827}, {"pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('yacht-hydrodynamics')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 9, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "last_rev": 39625, "last_value": 834.8990805149078}, {"pretty_name": "benchmarks.UserKnowledgeModelingSuite.track_acc('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.UserKnowledgeModelingSuite.track_acc", "last_rev": 39625, "last_value": 0.9615384340286255}, {"pretty_name": "benchmarks.UserKnowledgeModelingSuite.track_acc('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.UserKnowledgeModelingSuite.track_acc", "last_rev": 39625, "last_value": 0.9743589743589745}, {"pretty_name": "benchmarks.WineQualitySuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.WineQualitySuite.track_mae", "last_rev": 39625, "last_value": 0.4875065803527832}, {"pretty_name": "benchmarks.WineQualitySuite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.WineQualitySuite.track_mae", "last_rev": 39625, "last_value": 0.5111147334178289}, {"pretty_name": "benchmarks.WineSuite.track_acc('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.WineSuite.track_acc", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.WineSuite.track_acc('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.WineSuite.track_acc", "last_rev": 39625, "last_value": 1.0}, {"pretty_name": "benchmarks.YachtHydrodynamicsSuite.track_mae('appsheet')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 0, "name": "benchmarks.YachtHydrodynamicsSuite.track_mae", "last_rev": 39625, "last_value": 2.3854258060455322}, {"pretty_name": "benchmarks.YachtHydrodynamicsSuite.track_mae('automl_gs')", "last_err": 0.0, "prev_value": null, "change_rev": null, "idx": 1, "name": "benchmarks.YachtHydrodynamicsSuite.track_mae", "last_rev": 39625, "last_value": 2.0546305096533994}]