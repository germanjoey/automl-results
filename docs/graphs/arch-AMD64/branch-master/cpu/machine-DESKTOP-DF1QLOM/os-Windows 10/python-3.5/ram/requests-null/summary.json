[{"prev_value": 4.4317347294575455, "idx": 0, "name": "benchmarks.AbsenteeismSuite.track_mae", "pretty_name": "benchmarks.AbsenteeismSuite.track_mae('appsheet')", "last_value": 3.8552355250796757, "last_err": 0.0, "last_rev": 39625, "change_rev": [39607, 39625]}, {"prev_value": null, "idx": 1, "name": "benchmarks.AbsenteeismSuite.track_mae", "pretty_name": "benchmarks.AbsenteeismSuite.track_mae('automl_gs')", "last_value": 4.440629938701251, "last_err": 0.008895209243705793, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('nps')", "last_value": 0.6666666683943375, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('paper-reviews')", "last_value": 0.7804878048780488, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('user-knowledge-modeling')", "last_value": 0.9615384340286255, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.AccMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_appsheet('wine')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('nps')", "last_value": 0.5219512195121951, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('paper-reviews')", "last_value": 0.8442622950819673, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('user-knowledge-modeling')", "last_value": 0.9743589743589745, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.AccMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.AccMultiClassificationComparison.track_automl_gs('wine')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": 4.122581357156624, "idx": 0, "name": "benchmarks.AirflowSelfNoiseSuite.track_mae", "pretty_name": "benchmarks.AirflowSelfNoiseSuite.track_mae('appsheet')", "last_value": 4.5573069117716605, "last_err": 0.0, "last_rev": 39625, "change_rev": [39470, 39625]}, {"prev_value": null, "idx": 1, "name": "benchmarks.AirflowSelfNoiseSuite.track_mae", "pretty_name": "benchmarks.AirflowSelfNoiseSuite.track_mae('automl_gs')", "last_value": 3.9181944368958734, "last_err": 0.35375554074434623, "last_rev": 39625, "change_rev": null}, {"prev_value": 1.0, "idx": 0, "name": "benchmarks.AutismScreeningSuite.track_precision", "pretty_name": "benchmarks.AutismScreeningSuite.track_precision('appsheet')", "last_value": 0.95, "last_err": 0.0, "last_rev": 39625, "change_rev": [39607, 39625]}, {"prev_value": null, "idx": 1, "name": "benchmarks.AutismScreeningSuite.track_precision", "pretty_name": "benchmarks.AutismScreeningSuite.track_precision('automl_gs')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.AutismScreeningSuite.track_recall", "pretty_name": "benchmarks.AutismScreeningSuite.track_recall('appsheet')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.AutismScreeningSuite.track_recall", "pretty_name": "benchmarks.AutismScreeningSuite.track_recall('automl_gs')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.AutoMpgSuite.track_mae", "pretty_name": "benchmarks.AutoMpgSuite.track_mae('appsheet')", "last_value": 2.630856685149364, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.AutoMpgSuite.track_mae", "pretty_name": "benchmarks.AutoMpgSuite.track_mae('automl_gs')", "last_value": 2.17774829864502, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.ChurnPredictionSuite.track_precision", "pretty_name": "benchmarks.ChurnPredictionSuite.track_precision('appsheet')", "last_value": 0.8541666666666666, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.ChurnPredictionSuite.track_precision", "pretty_name": "benchmarks.ChurnPredictionSuite.track_precision('automl_gs')", "last_value": 0.9420889348500516, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.ChurnPredictionSuite.track_recall", "pretty_name": "benchmarks.ChurnPredictionSuite.track_recall('appsheet')", "last_value": 0.7592592592592593, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.ChurnPredictionSuite.track_recall", "pretty_name": "benchmarks.ChurnPredictionSuite.track_recall('automl_gs')", "last_value": 0.6905827787860456, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": 5.886489383676678, "idx": 0, "name": "benchmarks.ConcreteCalculatorSuite.track_mae", "pretty_name": "benchmarks.ConcreteCalculatorSuite.track_mae('appsheet')", "last_value": 7.354617081799553, "last_err": 0.0, "last_rev": 39625, "change_rev": [39470, 39625]}, {"prev_value": null, "idx": 1, "name": "benchmarks.ConcreteCalculatorSuite.track_mae", "pretty_name": "benchmarks.ConcreteCalculatorSuite.track_mae('automl_gs')", "last_value": 8.286293839945376, "last_err": 1.3628016502314528, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.DressesSalesSuite.track_precision", "pretty_name": "benchmarks.DressesSalesSuite.track_precision('appsheet')", "last_value": 0.4473684210526316, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.DressesSalesSuite.track_precision", "pretty_name": "benchmarks.DressesSalesSuite.track_precision('automl_gs')", "last_value": 0.5716501240694789, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.DressesSalesSuite.track_recall", "pretty_name": "benchmarks.DressesSalesSuite.track_recall('appsheet')", "last_value": 0.9444444444444444, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.DressesSalesSuite.track_recall", "pretty_name": "benchmarks.DressesSalesSuite.track_recall('automl_gs')", "last_value": 0.5528188286808977, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.EnergyEfficiency1Suite.track_mae", "pretty_name": "benchmarks.EnergyEfficiency1Suite.track_mae('appsheet')", "last_value": 2.549680010064856, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.EnergyEfficiency1Suite.track_mae", "pretty_name": "benchmarks.EnergyEfficiency1Suite.track_mae('automl_gs')", "last_value": 1.7529308504046817, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.EnergyEfficiency2Suite.track_mae", "pretty_name": "benchmarks.EnergyEfficiency2Suite.track_mae('appsheet')", "last_value": 2.4776498373452718, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.EnergyEfficiency2Suite.track_mae", "pretty_name": "benchmarks.EnergyEfficiency2Suite.track_mae('automl_gs')", "last_value": 1.3987778264825992, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.HeartDiseaseSuite.track_precision", "pretty_name": "benchmarks.HeartDiseaseSuite.track_precision('appsheet')", "last_value": 0.7391304347826086, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.HeartDiseaseSuite.track_precision", "pretty_name": "benchmarks.HeartDiseaseSuite.track_precision('automl_gs')", "last_value": 0.8164556962025317, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.HeartDiseaseSuite.track_recall", "pretty_name": "benchmarks.HeartDiseaseSuite.track_recall('appsheet')", "last_value": 0.8947368421052632, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.HeartDiseaseSuite.track_recall", "pretty_name": "benchmarks.HeartDiseaseSuite.track_recall('automl_gs')", "last_value": 0.8090243902439025, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('absenteeism')", "last_value": 3.8552355250796757, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('airflow-self-noise')", "last_value": 4.5573069117716605, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('auto-mpg')", "last_value": 2.630856685149364, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('concrete-calculator')", "last_value": 7.354617081799553, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('energy-efficiency-1')", "last_value": 2.549680010064856, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 5, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('energy-efficiency-2')", "last_value": 2.4776498373452718, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 6, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('student-performance-math')", "last_value": 1.8844806790351867, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 7, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('student-performance-por')", "last_value": 0.6685541006234976, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 8, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('wine-quality')", "last_value": 0.4875065803527832, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 9, "name": "benchmarks.MaeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.MaeRegressionComparison.track_appsheet('yacht-hydrodynamics')", "last_value": 2.3854258060455322, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('absenteeism')", "last_value": 4.449525147944957, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('airflow-self-noise')", "last_value": 3.7958809229814814, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('auto-mpg')", "last_value": 2.17774829864502, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('concrete-calculator')", "last_value": 10.233697015212963, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('energy-efficiency-1')", "last_value": 1.7529308504046817, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 5, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('energy-efficiency-2')", "last_value": 1.3987778264825992, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 6, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('student-performance-math')", "last_value": 4.42217900259655, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 7, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('student-performance-por')", "last_value": 1.3589764619484923, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 8, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('wine-quality')", "last_value": 0.5111147334178289, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 9, "name": "benchmarks.MaeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.MaeRegressionComparison.track_automl_gs('yacht-hydrodynamics')", "last_value": 2.0546305096533994, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.NpsSuite.track_acc", "pretty_name": "benchmarks.NpsSuite.track_acc('appsheet')", "last_value": 0.6666666683943375, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.NpsSuite.track_acc", "pretty_name": "benchmarks.NpsSuite.track_acc('automl_gs')", "last_value": 0.5219512195121951, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.PaperReviewsSuite.track_acc", "pretty_name": "benchmarks.PaperReviewsSuite.track_acc('appsheet')", "last_value": 0.7804878048780488, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.PaperReviewsSuite.track_acc", "pretty_name": "benchmarks.PaperReviewsSuite.track_acc('automl_gs')", "last_value": 0.8442622950819673, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('autism-screening')", "last_value": 0.95, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_value": 0.8541666666666666, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_value": 0.4473684210526316, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('heart-disease')", "last_value": 0.7391304347826086, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_appsheet('titanic')", "last_value": 0.717391304347826, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_value": 0.9420889348500516, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_value": 0.5716501240694789, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_value": 0.8164556962025317, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.PrecisionBinaryClassificationComparison.track_automl_gs('titanic')", "last_value": 0.8185328185328185, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('autism-screening')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_value": 0.7592592592592593, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_value": 0.9444444444444444, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('heart-disease')", "last_value": 0.8947368421052632, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_appsheet('titanic')", "last_value": 0.9166666666666666, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_value": 0.6905827787860456, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_value": 0.5528188286808977, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_value": 0.8090243902439025, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.RecallBinaryClassificationComparison.track_automl_gs('titanic')", "last_value": 0.7222418358340689, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.StudentPerformanceMathSuite.track_mae", "pretty_name": "benchmarks.StudentPerformanceMathSuite.track_mae('appsheet')", "last_value": 1.8844806790351867, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.StudentPerformanceMathSuite.track_mae", "pretty_name": "benchmarks.StudentPerformanceMathSuite.track_mae('automl_gs')", "last_value": 4.42217900259655, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.StudentPerformancePorSuite.track_mae", "pretty_name": "benchmarks.StudentPerformancePorSuite.track_mae('appsheet')", "last_value": 0.6685541006234976, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.StudentPerformancePorSuite.track_mae", "pretty_name": "benchmarks.StudentPerformancePorSuite.track_mae('automl_gs')", "last_value": 1.3589764619484923, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TitanicSuite.track_precision", "pretty_name": "benchmarks.TitanicSuite.track_precision('appsheet')", "last_value": 0.717391304347826, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TitanicSuite.track_precision", "pretty_name": "benchmarks.TitanicSuite.track_precision('automl_gs')", "last_value": 0.8185328185328185, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TitanicSuite.track_recall", "pretty_name": "benchmarks.TitanicSuite.track_recall('appsheet')", "last_value": 0.9166666666666666, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TitanicSuite.track_recall", "pretty_name": "benchmarks.TitanicSuite.track_recall('automl_gs')", "last_value": 0.7222418358340689, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('autism-screening')", "last_value": 32.047656297683716, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('churn-prediction')", "last_value": 43.55696892738342, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('dresses-sales')", "last_value": 31.207871198654175, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('heart-disease')", "last_value": 28.1053524017334, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_appsheet('titanic')", "last_value": 27.223865509033203, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('autism-screening')", "last_value": 2609.7796580791473, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('churn-prediction')", "last_value": 2399.239366531372, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('dresses-sales')", "last_value": 1567.8775324821472, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('heart-disease')", "last_value": 2765.232954263687, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeBinaryClassificationComparison.track_automl_gs('titanic')", "last_value": 2645.2976908683777, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('nps')", "last_value": 28.66774344444275, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('paper-reviews')", "last_value": 30.3750216960907, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('user-knowledge-modeling')", "last_value": 29.761793851852417, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_appsheet('wine')", "last_value": 28.168158531188965, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('nps')", "last_value": 1141.703991651535, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('paper-reviews')", "last_value": 1846.0559742450714, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('user-knowledge-modeling')", "last_value": 1324.8651053905487, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeMultiClassificationComparison.track_automl_gs('wine')", "last_value": 2235.147046804428, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('absenteeism')", "last_value": 23.939385414123535, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('airflow-self-noise')", "last_value": 39.72560095787048, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('auto-mpg')", "last_value": 30.665636777877808, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('concrete-calculator')", "last_value": 23.945170402526855, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('energy-efficiency-1')", "last_value": 30.47210931777954, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 5, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('energy-efficiency-2')", "last_value": 23.35370922088623, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 6, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('student-performance-math')", "last_value": 28.126938819885254, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 7, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('student-performance-por')", "last_value": 29.535085916519165, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 8, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('wine-quality')", "last_value": 32.21105217933655, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 9, "name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_appsheet('yacht-hydrodynamics')", "last_value": 28.3132061958313, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('absenteeism')", "last_value": 1720.7366738319397, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('airflow-self-noise')", "last_value": 1000.5141520500183, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 2, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('auto-mpg')", "last_value": 2555.131772994995, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 3, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('concrete-calculator')", "last_value": 1688.6441690921783, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 4, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('energy-efficiency-1')", "last_value": 1253.6088423728943, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 5, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('energy-efficiency-2')", "last_value": 1532.7790684700012, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 6, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('student-performance-math')", "last_value": 1044.400455713272, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 7, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('student-performance-por')", "last_value": 686.2359955310822, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 8, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('wine-quality')", "last_value": 1842.6598403453827, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 9, "name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs", "pretty_name": "benchmarks.TrainingTimeRegressionComparison.track_automl_gs('yacht-hydrodynamics')", "last_value": 834.8990805149078, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.UserKnowledgeModelingSuite.track_acc", "pretty_name": "benchmarks.UserKnowledgeModelingSuite.track_acc('appsheet')", "last_value": 0.9615384340286255, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.UserKnowledgeModelingSuite.track_acc", "pretty_name": "benchmarks.UserKnowledgeModelingSuite.track_acc('automl_gs')", "last_value": 0.9743589743589745, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.WineQualitySuite.track_mae", "pretty_name": "benchmarks.WineQualitySuite.track_mae('appsheet')", "last_value": 0.4875065803527832, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.WineQualitySuite.track_mae", "pretty_name": "benchmarks.WineQualitySuite.track_mae('automl_gs')", "last_value": 0.5111147334178289, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.WineSuite.track_acc", "pretty_name": "benchmarks.WineSuite.track_acc('appsheet')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.WineSuite.track_acc", "pretty_name": "benchmarks.WineSuite.track_acc('automl_gs')", "last_value": 1.0, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 0, "name": "benchmarks.YachtHydrodynamicsSuite.track_mae", "pretty_name": "benchmarks.YachtHydrodynamicsSuite.track_mae('appsheet')", "last_value": 2.3854258060455322, "last_err": 0.0, "last_rev": 39625, "change_rev": null}, {"prev_value": null, "idx": 1, "name": "benchmarks.YachtHydrodynamicsSuite.track_mae", "pretty_name": "benchmarks.YachtHydrodynamicsSuite.track_mae('automl_gs')", "last_value": 2.0546305096533994, "last_err": 0.0, "last_rev": 39625, "change_rev": null}]